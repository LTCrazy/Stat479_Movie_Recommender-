{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie Recommendation System_Group5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1ciLw2y3auyq",
        "NWzZPFMk7FB0",
        "8XD5geVx7jkE",
        "GEQjcnFiQzUQ",
        "MtCG41OKQ8Ow",
        "jCOtl9Xea-X6",
        "Z22KPkdgcXL5",
        "ENzDPcFBegE2"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HemeFLrFK4P7",
        "colab_type": "text"
      },
      "source": [
        "# Shixuan Song\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQXzOkJqLCUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load packages\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JnIogCwUv_t",
        "colab_type": "code",
        "outputId": "d90a3cd0-cc38-4b1a-ff0c-28253546b75d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De4aVCfWfWig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load data\n",
        "all_data = pd.read_csv(\"top150SplitCompanies.csv\" ,index_col = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDBLL33Mfbzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_data['class'] = np.where(all_data['rating']< 4, 0, 1)\n",
        "y = all_data['class']\n",
        "drop_columns = ['class', 'userId']\n",
        "X = all_data.drop(labels= drop_columns, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkWFw2zOZzho",
        "colab_type": "text"
      },
      "source": [
        "###KNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-FQQgPJfu_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#load data\n",
        "all_data = pd.read_csv(\"top150SplitCompanies.csv\" ,index_col = 0)\n",
        "all_data = all_data.loc[:, 'budget':'Documentary']\n",
        "\n",
        "def knn_model_acc(user):\n",
        "    user_data = all_data.loc[all_data['userId'] == user] #test with user 414\n",
        "    user_data['class'] = np.where(user_data['rating']< 4, 0, 1)\n",
        "    y = user_data['class']\n",
        "    drop_columns = ['class', 'userId']\n",
        "    X = user_data.drop(labels= drop_columns, axis=1)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=223,\n",
        "                                                    shuffle=True,\n",
        "                                                    stratify = y)\n",
        "    X_train = scale(X_train)\n",
        "    X_test = scale(X_test)\n",
        "    grid_params_knn = {\n",
        "        'n_neighbors':[*range(2,15,1)],\n",
        "        'weights':['uniform']\n",
        "    }\n",
        "\n",
        "    gs_knn = GridSearchCV(\n",
        "         KNeighborsClassifier(),\n",
        "         grid_params_knn,\n",
        "         verbose = 1,\n",
        "         cv = 10,\n",
        "         refit=True,\n",
        "         scoring='accuracy',\n",
        "         n_jobs = -1)\n",
        "\n",
        "    gs_knn_results = gs_knn.fit(X_train, y_train)\n",
        "    knn_best_score = gs_knn.best_estimator_.score(X_test, y_test)*100\n",
        "    #print('UserId:', userid, 'Accuracy:',knn_best_score)\n",
        "    return knn_best_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQCRA__-fvXw",
        "colab_type": "text"
      },
      "source": [
        "calculate KNN model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMHFxGLAf1Q5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "userId_list = all_data.userId.unique()\n",
        "accuracy_list = []\n",
        "for i in userId_list:\n",
        "    accuracy_list.append(knn_model_acc(user = i))\n",
        "    \n",
        "print(\"In KNN model, the average accuracy for all users is \", round(np.mean(accuracy_list),2),\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XXWlai2umaW",
        "colab_type": "text"
      },
      "source": [
        "###Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MjykGdskZxa7",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "\n",
        "def gb_model_acc(user):\n",
        "    user_data = all_data.loc[all_data['userId'] == user] #test with user 414\n",
        "    user_data['class'] = np.where(user_data['rating']< 4, 0, 1)\n",
        "    y = user_data['class']\n",
        "    drop_columns = ['class', 'userId']\n",
        "    X = user_data.drop(labels= drop_columns, axis=1)\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=223,\n",
        "                                                    shuffle=True,\n",
        "                                                    stratify = y)\n",
        "    \n",
        "    grid_params_gb = [{\n",
        "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1],\n",
        "    \"max_depth\": [1, 2, 3, 4, 5, 10, 15, 20, None],\n",
        "    #\"criterion\": [\"gini\",  \"entropy\"],\n",
        "    \"n_estimators\":[5,10]\n",
        "    }]\n",
        "\n",
        "    gs_gb = GridSearchCV(\n",
        "         GradientBoostingClassifier(),\n",
        "         grid_params_gb,\n",
        "         cv = 10,\n",
        "         n_jobs = -1)\n",
        "\n",
        "    gs_gb_results = gs_gb.fit(X_train, y_train)\n",
        "    gb_best_score = gs_gb.best_estimator_.score(X_test, y_test)*100\n",
        "    #print('UserId:', userid, 'Accuracy:',knn_best_score)\n",
        "    return gb_best_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTdcqPipgPxR",
        "colab_type": "text"
      },
      "source": [
        "Calculate average accuracy of KNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQFCv8bfgKs_",
        "colab_type": "code",
        "outputId": "cf04a2f1-f037-4563-fff2-5476865d15fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "userId_list = all_data.userId.unique()\n",
        "accuracy_list = []\n",
        "for i in userId_list:\n",
        "    accuracy_list.append(gb_model_acc(user = i))\n",
        "    \n",
        "print(\"In gradient boosting, the average accuracy for all users is \", round(np.mean(accuracy_list),2),\"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b10b1545d9db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0maccuracy_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muserId_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0maccuracy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgb_model_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In gradient boosting, the average accuracy for all users is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gb_model_acc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ciLw2y3auyq",
        "colab_type": "text"
      },
      "source": [
        "# Tianchang Li\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWzZPFMk7FB0",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4U4ZHKTbE80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################### Random Forest Classifier ##################################\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def RandomForest(X_train, y_train, X_test, y_test):\n",
        "    \n",
        "    ## Tuning hyperparameters\n",
        "    param_grid = [{'max_depth': [5, 8, 10, 15, None],\n",
        "                   'n_estimators': [120, 150, 180],\n",
        "                   'criterion': ['gini','entropy']}]\n",
        "\n",
        "    rd = GridSearchCV(estimator=RandomForestClassifier(random_state=223),\n",
        "                      param_grid=param_grid,\n",
        "                      n_jobs=-1,\n",
        "                      scoring = 'accuracy',\n",
        "                      refit=True,\n",
        "                      cv=10)\n",
        "    ## \"refit\" auto fit to the optimal model\n",
        "    rd.fit(X_train, y_train)\n",
        "\n",
        "    print('Best Accuracy: %.2f%%' % (rd.best_score_*100))\n",
        "    print('Best Params:', rd.best_params_)\n",
        "    print('Test Accuracy: %.2f%%' % (rd.best_estimator_.score(X_test, y_test)*100))\n",
        "\n",
        "    # Feature selection for Random Forest\n",
        "    imp = rd.best_estimator_.feature_importances_\n",
        "    col_selected = topUser.drop(columns = ['rating','userId','vote_average','vote_count']).columns[imp > 0.001]\n",
        "    X_selected = np.array(topUser[col_selected])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y,\n",
        "                                                        test_size=0.3, \n",
        "                                                        random_state=223,\n",
        "                                                        shuffle=True,\n",
        "                                                        stratify = y)\n",
        "\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=10, random_state=223, shuffle=True)\n",
        "    kfold_acc = 0.\n",
        "\n",
        "    for train_idx, valid_idx in cv.split(X_train, y_train):\n",
        "        clf = RandomForestClassifier(criterion=rd.best_params_.get('criterion'),max_depth=rd.best_params_.get('max_depth'),n_estimators=rd.best_params_.get('n_estimators')).fit(X_train[train_idx], y_train[train_idx])\n",
        "        y_pred = clf.predict(X_train[valid_idx])\n",
        "        acc = np.mean(y_pred == y_train[valid_idx])*100\n",
        "        kfold_acc += acc\n",
        "    kfold_acc /= 10\n",
        "\n",
        "    clf = RandomForestClassifier(criterion=rd.best_params_.get('criterion'),max_depth=rd.best_params_.get('max_depth'),n_estimators=rd.best_params_.get('n_estimators')).fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    test_acc = np.mean(y_pred == y_test)*100\n",
        "\n",
        "    print('Kfold Accuracy: %.2f%%' % kfold_acc)\n",
        "    print('Test Accuracy: %.2f%%' % test_acc)\n",
        "    \n",
        "    return np.mean(y_pred == y_test)\n",
        "    \n",
        "    \n",
        "user150 = pd.read_csv(\"top150SplitCompanies.csv\")\n",
        "user150.drop(columns = ['Unnamed: 0'], axis = 1, inplace = True)\n",
        "# Loop through all users\n",
        "userId_list = user150['userId'].unique()\n",
        "accuracy_list = []\n",
        "for elem in userId_list:\n",
        "    topUser = user150[user150['userId'] == elem]\n",
        "    X = np.array(topUser.drop(columns = ['rating','userId','vote_average','vote_count']))\n",
        "    y = np.array(topUser['rating'])\n",
        "    y[y < 4] = 0\n",
        "    y[y >= 4] = 1\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=223,\n",
        "                                                    shuffle=True,\n",
        "                                                    stratify = y)\n",
        "    accuracy_list.append(RandomForest(X_train, y_train, X_test, y_test))\n",
        "    \n",
        "mean_acc = np.mean(accuracy_list)  # Calculate average accuracy of all the individual user model\n",
        "print(\"The average accurary of Random Forest is : %.2f%%\" % (mean_acc*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XD5geVx7jkE",
        "colab_type": "text"
      },
      "source": [
        "### Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTqVRYHS7o-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################### Decision Tree ###################################\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def DecisionTree(X_train, y_train, X_test, y_test):\n",
        "    \n",
        "    param_grid = [{'max_depth': [3, 5, 7, 10, 13, None],'criterion': ['gini','entropy']}]\n",
        "    dt = GridSearchCV(estimator=DecisionTreeClassifier(random_state=223),\n",
        "                      param_grid=param_grid,\n",
        "                      iid = True,\n",
        "                      n_jobs=-1,\n",
        "                      refit=True,\n",
        "                      cv=10)\n",
        "    ## \"refit\" auto fit to the optimal model\n",
        "    dt.fit(X_train, y_train)\n",
        "\n",
        "    print('Best Accuracy: %.2f%%' % (dt.best_score_*100))\n",
        "    print('Best Params:', dt.best_params_)\n",
        "    print('Test Accuracy: %.2f%%' % (dt.best_estimator_.score(X_test, y_test)*100))\n",
        "#     PlotTree(X_train, y_train,dt)\n",
        "    \n",
        "    return dt.best_estimator_.score(X_test, y_test)\n",
        "    \n",
        "## Plot decision tree (Called at the end DecisionTree function)\n",
        "def PlotTree(X_train,y_train,dt):\n",
        "    from sklearn.tree import export_graphviz\n",
        "    from sklearn.externals.six import StringIO  \n",
        "    from IPython.display import Image  \n",
        "    import pydotplus\n",
        "\n",
        "    feature_cols = topUser.drop(columns = ['rating','userId','vote_average','vote_count']).columns\n",
        "    clf = dt.best_estimator_\n",
        "    clf.fit(X_train,y_train)\n",
        "\n",
        "    dot_data = StringIO()\n",
        "    export_graphviz(clf, out_file=dot_data,  \n",
        "                    filled=True, rounded=True,\n",
        "                    special_characters=True,\n",
        "                    feature_names = feature_cols,\n",
        "                    class_names=['Not recommend','Recommend'])\n",
        "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "    graph.write_png('DT_max5.png')\n",
        "    Image(graph.create_png())\n",
        "\n",
        "    \n",
        "user150 = pd.read_csv(\"top150SplitCompanies.csv\")\n",
        "user150.drop(columns = ['Unnamed: 0'], axis = 1, inplace = True)\n",
        "# Loop through all users\n",
        "userId_list = user150['userId'].unique()\n",
        "accuracy_list = []\n",
        "for elem in userId_list:\n",
        "    topUser = user150[user150['userId'] == elem]\n",
        "    X = np.array(topUser.drop(columns = ['rating','userId','vote_average','vote_count']))\n",
        "    y = np.array(topUser['rating'])\n",
        "    y[y < 4] = 0\n",
        "    y[y >= 4] = 1\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=223,\n",
        "                                                    shuffle=True,\n",
        "                                                    stratify = y)\n",
        "    accuracy_list.append(DecisionTree(X_train, y_train, X_test, y_test))\n",
        "    \n",
        "mean_acc = np.mean(accuracy_list)  # Calculate average accuracy of all the individual user model\n",
        "print(\"The average accurary of Decision tree is : %.2f%%\" % (mean_acc*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEQjcnFiQzUQ",
        "colab_type": "text"
      },
      "source": [
        "### K-Mean Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0t147krQ4Q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################## K-Mean Clustering ##################################\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Loop through each user\n",
        "def K_Mean (X_train, y_train):\n",
        "    # Select the best \"K\"\n",
        "    distortions = []\n",
        "    for i in range(1, 15):\n",
        "        km = KMeans(n_clusters=i, \n",
        "                    random_state=233)\n",
        "        km.fit(X_train)\n",
        "        distortions.append(km.inertia_)\n",
        "    plt.plot(range(1, 15), distortions, marker='o')\n",
        "    plt.xlabel('Number of clusters')\n",
        "    plt.ylabel('Distortion')\n",
        "#     plt.savefig('distortion with K means',dpi = 300)\n",
        "    \n",
        "    ## Apply number of clusters as 4\n",
        "    nclusters = 4\n",
        "    kmeans = KMeans(n_clusters=nclusters, random_state=233)\n",
        "    labels = kmeans.fit_predict(X_train)\n",
        "    for i in range(nclusters):\n",
        "        print('Mean of cluster_',i,':',np.mean(y_train[labels == i]))\n",
        "\n",
        "    n_clus = len(np.unique(labels))\n",
        "    n_recom = 0\n",
        "    true_mean = []\n",
        "    # Cluster level info\n",
        "    for i in range(n_clus):\n",
        "        true_mean.append(np.mean(y_train[labels == i]))\n",
        "        print('Mean of cluster_',i,':',np.mean(y_train[labels == i]))\n",
        "        if(np.mean(y_train[labels == i])>3.75): \n",
        "            n_recom += np.sum(labels == i)\n",
        "            labels[labels == i] = 1   \n",
        "        else:\n",
        "            labels[labels == i] = 0\n",
        "\n",
        "    y_label = y_train >= 4\n",
        "    y_label = y_label.astype(int)\n",
        "    acc = accuracy_score(y_label, labels)\n",
        "    print('Accuracy score: %.2f%%' % (acc*100))\n",
        "    print(confusion_matrix(y_label, labels))\n",
        "    \n",
        "    return (acc)\n",
        "\n",
        "\n",
        "\n",
        "user150 = pd.read_csv(\"top150SplitCompanies.csv\")\n",
        "user150.drop(columns = ['Unnamed: 0'], axis = 1, inplace = True)\n",
        "# Loop through all users\n",
        "userId_list = user150['userId'].unique()\n",
        "accuracy_list = []\n",
        "for elem in userId_list:\n",
        "    topUser = user150[user150['userId'] == elem]\n",
        "    X = np.array(topUser.drop(columns = ['rating','userId']))\n",
        "    y = np.array(topUser['rating'])\n",
        "\n",
        "    # Rescaling\n",
        "    scaler = StandardScaler()\n",
        "    X_trans = scaler.fit_transform(X)\n",
        "    \n",
        "    accuracy_list.append(K_Mean(X_trans, y))\n",
        "    \n",
        "mean_acc = np.mean(accuracy_list)  # Calculate average accuracy of all the individual user model\n",
        "print(\"The average accurary of K-Mean clustering is : %.2f%%\" % (mean_acc*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtCG41OKQ8Ow",
        "colab_type": "text"
      },
      "source": [
        "### Mean Shift Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fW0ajjERAVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################## Mean shift method ##############################\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def MeanShift(X_trans, y_train, X_test, y_test):\n",
        "    \n",
        "    bandwidth = estimate_bandwidth(X_trans, quantile=0.3)\n",
        "    shift = MeanShift(bandwidth = bandwidth)\n",
        "    shift.fit(X_trans)\n",
        "\n",
        "    pred_label = shift.predict(X_test)\n",
        "    pred_level = []\n",
        "    for i in range(len(pred_label)):\n",
        "        pred_level.append(true_mean[pred_label[i]])\n",
        "    pred_level = np.array(pred_level)\n",
        "    pred_level[pred_level < 3.75] = 0\n",
        "    pred_level[pred_level >= 3.75] = 1\n",
        "    y_label = y_test >= 4\n",
        "    y_label = y_label.astype(int)\n",
        "    # print('adjusted score:',adjusted_rand_score(y_label, pred_level))\n",
        "    # print('Accuracy score: %.2f%%', accuracy_score(y_label, pred_level)*100)\n",
        "    # fpr = np.sum(y_label[pred_level == 1] !=1)/np.sum(y_label[pred_level == 1] ==1)*100\n",
        "    # fnr = np.sum(y_label[pred_level == 0] !=0)/np.sum(y_label[pred_level == 0] ==0)*100\n",
        "    # print(\"false positive rate : %.2f%%\" % fpr)\n",
        "    # print(\"false negative rate : %.2f%%\" % fnr)\n",
        "\n",
        "    confmat = confusion_matrix(y_label, pred_level)\n",
        "    fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
        "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
        "    for i in range(confmat.shape[0]):\n",
        "        for j in range(confmat.shape[1]):\n",
        "            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
        "\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "    plt.tight_layout()\n",
        "    # plt.savefig('MeanShift_confmat.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy_score(y_label, pred_level)\n",
        "\n",
        "\n",
        "# Split training, test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "user150 = pd.read_csv(\"top150SplitCompanies.csv\")\n",
        "user150.drop(columns = ['Unnamed: 0'], axis = 1, inplace = True)\n",
        "# Loop through all users\n",
        "userId_list = user150['userId'].unique()\n",
        "accuracy_list = []\n",
        "for elem in userId_list:\n",
        "    topUser = user150[user150['userId'] == elem]\n",
        "    X = np.array(topUser.drop(columns = ['rating','userId']))\n",
        "    y = np.array(topUser['rating'])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=233,\n",
        "                                                    shuffle=True)\n",
        "    \n",
        "    # Rescaling\n",
        "    scaler = StandardScaler()\n",
        "    X_trans = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.fit_transform(X_train)\n",
        "    \n",
        "    accuracy_list.append(MeanShift(X_train, y_train, X_test, y_test))\n",
        "    \n",
        "mean_acc = np.mean(accuracy_list)  # Calculate average accuracy of all the individual user model\n",
        "print(\"The average accurary of Mean Shift clustering is : %.2f%%\" % (mean_acc*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmBv7w-mRG38",
        "colab_type": "text"
      },
      "source": [
        "### DBSCAN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3Vay3WoRKP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################## DBSCAN ###########################\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def DBSCAN(X_trans, y_train):\n",
        "    \n",
        "    db = DBSCAN(eps=1, min_samples=2).fit(X_trans)\n",
        "    labels = db.labels_\n",
        "    n_clus = len(np.unique(labels))\n",
        "    n_recom = 0\n",
        "    true_mean = []\n",
        "    # Cluster level info\n",
        "    for i in range(-1,n_clus-1):\n",
        "        if i == -1:\n",
        "            true_mean.append(np.mean(y_train[labels == i]))\n",
        "            print('Mean of cluster_',i,':',np.mean(y_train[labels == i]))\n",
        "            labels[labels == i] = 0\n",
        "        else:\n",
        "            true_mean.append(np.mean(y_train[labels == i]))\n",
        "            print('Mean of cluster_',i,':',np.mean(y_train[labels == i]))\n",
        "            if(np.mean(y_train[labels == i])>3.75): \n",
        "                n_recom += np.sum(labels == i)\n",
        "                labels[labels == i] = 1   \n",
        "            else:\n",
        "                labels[labels == i] = 0\n",
        "\n",
        "\n",
        "    y_label = y_train >= 4\n",
        "    y_label = y_label.astype(int)\n",
        "    # print('adjusted score:',adjusted_rand_score(y_label, labels))\n",
        "    acc = accuracy_score(y_label, labels)\n",
        "    print('Accuracy score: %.2f%%' % (acc*100))\n",
        "    # print(\"number of estimated clusters : %d\" % n_clus)\n",
        "    # print(\"number of recommended ones : %d\" % n_recom)\n",
        "    # fpr = np.sum(y_label[labels == 1] !=1)/np.sum(y_label[labels == 1] ==1)*100\n",
        "    # fnr = np.sum(y_label[labels == 0] !=0)/np.sum(y_label[labels == 0] ==0)*100\n",
        "    # print(\"false positive rate : %.2f%%\" % fpr)\n",
        "    # print(\"false negative rate : %.2f%%\" % fnr)\n",
        "\n",
        "    confmat = confusion_matrix(y_label, labels)\n",
        "    fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
        "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
        "    for i in range(confmat.shape[0]):\n",
        "        for j in range(confmat.shape[1]):\n",
        "            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
        "\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # plt.savefig('DBSCAN_confmat.png', dpi=300)\n",
        "    plt.show()\n",
        "    \n",
        "    return acc\n",
        "\n",
        "    \n",
        "user150 = pd.read_csv(\"top150SplitCompanies.csv\")\n",
        "user150.drop(columns = ['Unnamed: 0'], axis = 1, inplace = True)\n",
        "# Loop through all users\n",
        "userId_list = user150['userId'].unique()\n",
        "accuracy_list = []\n",
        "for elem in userId_list:\n",
        "    topUser = user150[user150['userId'] == elem]\n",
        "    X = np.array(topUser.drop(columns = ['rating','userId']))\n",
        "    y = np.array(topUser['rating'])\n",
        "\n",
        "    # Rescaling\n",
        "    scaler = StandardScaler()\n",
        "    X_trans = scaler.fit_transform(X)\n",
        "    \n",
        "    accuracy_list.append(DBSCAN(X_trans, y))\n",
        "    \n",
        "mean_acc = np.mean(accuracy_list)  # Calculate average accuracy of all the individual user model\n",
        "print(\"The average accurary of DBSCAN clustering is : %.2f%%\" % (mean_acc*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCOtl9Xea-X6",
        "colab_type": "text"
      },
      "source": [
        "# Maoze Wang"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5B5FDpibADr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbiFTEJcU44o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('top150SplitCompanies.csv')  # Read dataset\n",
        "#  Transform rating to binary 0 and 1\n",
        "data['rating'][data['rating'] < 3.75] = 0  # Rating smaller than 3.75 will sorted to 'dislike(0)'\n",
        "data['rating'][data['rating'] > 3.75] = 1  # Rating larger than 3.75 will sorted to 'like(1)'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRn5a9ySVJkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split train and test dataset\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def my_train_test_split(userId_input):\n",
        "    data_user = data.loc[data['userId'] == userId_input]  # Select User\n",
        "    y = data_user[['rating']]  # Result\n",
        "    X = data_user[['budget', 'original_language','popularity','year_from_now','revenue','runtime','status','vote_count',\n",
        "              'Count_movies','Drama','Sci-Fi','Action','Crime','Thriller','Comedy','Horror','Adventure','Animation','Fantasy',\n",
        "              'Musical', 'Romance','Mystery', 'IMAX','Western','War', 'Children','Documentary','Film-Noir', 'DE', 'US','JP','FR','GB',\n",
        "              'AU','CN','IT','IE','CA','KR','CZ','BE','ES','RO','ZA','IN','IS','MX','HK','LU','TW','SG','MA','MT','JM','BS','DM','NZ',\n",
        "              'CH','FJ', 'BR','AR','EC','CO','DK','RU','HU','NL','AE','EG','FI','AT','PK','PH','SE','IL','PL','CS','NO','MY','UA','AO',\n",
        "              'CL','AF','TH','IR','MC','BG','PE','SK','GR','LT']]  # Variables\n",
        "    X_train_sub, X_val, y_train_sub, y_val = train_test_split(X, y, test_size = 0.2, random_state = 123)  # Split data from same user with 0.2 test dataset.\n",
        "    split_result = [X_train_sub, X_val, y_train_sub, y_val]  # A value that contain X train, test data and y train,test data.\n",
        "    return split_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foz0RL35VQW-",
        "colab_type": "text"
      },
      "source": [
        "1) Individual user models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ENqiRiJVKvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create logistic lasso regression model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "def logistic_lasso(X_train, y_train, X_test, y_test):\n",
        "    log_l1 = LogisticRegression(penalty='l1', solver='liblinear')  # Create model with Logistic regression\n",
        "    log_l1.fit(X_train,y_train)  # Fit the model with train data\n",
        "    y_pred = log_l1.predict(X_test)  # Predict the test data\n",
        "    acc = accuracy_score(y_test, y_pred)  # Calculate accuracy score\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SyLOqsJrheO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "userId_list = data['userId'].unique()\n",
        "accuracy_list = []\n",
        "for elem in userId_list:\n",
        "    X_train = my_train_test_split(elem)[0]\n",
        "    y_train = my_train_test_split(elem)[2]\n",
        "    X_test = my_train_test_split(elem)[1]\n",
        "    y_test = my_train_test_split(elem)[3]\n",
        "    accuracy_list.append(logistic_lasso(X_train, y_train, X_test, y_test))\n",
        "mean_acc = np.mean(accuracy_list)\n",
        "print(\"In Lasso Regression model, the average accuracy for all users is \", round(mean_acc*100,2),\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRfN-RPfrqKz",
        "colab_type": "text"
      },
      "source": [
        "2) Detail logistic lasso regression model for user 414"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z40I6kajrlvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a logistic regression for overall data for user 414 only\n",
        "data_414 = data.loc[data['userId'] == 414]\n",
        "y = data_414[['rating']]  # Result\n",
        "X = data_414[['budget', 'original_language','popularity','year_from_now','revenue','runtime','status','vote_count',\n",
        "          'Count_movies','Drama','Sci-Fi','Action','Crime','Thriller','Comedy','Horror','Adventure','Animation','Fantasy',\n",
        "          'Musical', 'Romance','Mystery', 'IMAX','Western','War', 'Children','Documentary','Film-Noir', 'DE', 'US','JP','FR','GB',\n",
        "          'AU','CN','IT','IE','CA','KR','CZ','BE','ES','RO','ZA','IN','IS','MX','HK','LU','TW','SG','MA','MT','JM','BS','DM','NZ',\n",
        "          'CH','FJ', 'BR','AR','EC','CO','DK','RU','HU','NL','AE','EG','FI','AT','PK','PH','SE','IL','PL','CS','NO','MY','UA','AO',\n",
        "          'CL','AF','TH','IR','MC','BG','PE','SK','GR','LT']]  # Variables\n",
        "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X, y, test_size = 0.2, random_state = 123,stratify = y,shuffle = True)  # Split whole data into train test dataset\n",
        "\n",
        "log_l1 = LogisticRegression(penalty='l1', solver='liblinear')  # Create model with Lasso regression\n",
        "log_l1.fit(X_train_sub,y_train_sub)  # Fit the model with train data\n",
        "y_pred = log_l1.predict(X_val)  # Predict the test data\n",
        "acc = accuracy_score(y_val, y_pred)  # Calculate accuracy score\n",
        "print(\"In Lasso Regression model, the accuracy for user 414 is\", round(acc*100,2),\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghy0f13SVOcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(log_l1.coef_)  # Print the coefficients of model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD6KUquNr0eC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert coefficients to change in probability\n",
        "import math\n",
        "print('budget: ' + str(math.exp(-1.70989788e-08)/(1 + math.exp(-1.70989788e-08))))  # budget\n",
        "print('original_language: ' + str(math.exp(4.97632901e-02)/(1 + math.exp(4.97632901e-02))))  # original_language\n",
        "print('popularity: ' + str(math.exp(1.55152778e-02)/(1 + math.exp(1.55152778e-02))))  # popularity\n",
        "print('year_from_now: ' + str(math.exp(1.96388026e-02)/(1 + math.exp(1.96388026e-02))))  # year_from_now\n",
        "print('revenue: ' + str(math.exp(-6.49735709e-10)/(1 + math.exp(-6.49735709e-10))))  # revenue\n",
        "print('runtime: ' + str(math.exp(1.42996047e-02)/(1 + math.exp( 1.42996047e-02))))  # runtime\n",
        "print('vote_count: ' + str(math.exp(4.98632801e-04)/(1 + math.exp(4.98632801e-04))))  # vote_count\n",
        "print('Count_movies: ' + str(math.exp( -2.03109209e-03)/(1 + math.exp( -2.03109209e-03))))  # Count_movies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf5FSZ-DsCe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "confmat = confusion_matrix(y_val, y_pred)  # Create confusion matrix\n",
        "confmat\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.8)\n",
        "for i in range(confmat.shape[0]):\n",
        "    for j in range(confmat.shape[1]):\n",
        "        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
        "\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "\n",
        "plt.tight_layout()\n",
        "#plt.savefig('images/06_09.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Print accuracy, precision, and recall\n",
        "print(\"Accuracy:\",accuracy_score(y_val, y_pred))\n",
        "print(\"Precision:\",precision_score(y_val, y_pred))\n",
        "print(\"Recall:\",recall_score(y_val, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSV_V3sFsGTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot ROC Curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "y_pred_proba = log_l1.predict_proba(X_val)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_val,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_val, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"logistic lasso, auc=\"+str(auc))\n",
        "plt.plot([0, 1],[0, 1],linestyle='--',color=(0.6, 0.6, 0.6),label='Random guessing')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z22KPkdgcXL5",
        "colab_type": "text"
      },
      "source": [
        "# Xuchen Xue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQxQJ7HqePfQ",
        "colab_type": "text"
      },
      "source": [
        "###Build tree regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omzVH1TAUbpJ",
        "colab_type": "code",
        "outputId": "5ab4f0fd-11ef-47ca-992f-227b29c6f6d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "import numpy as np   \n",
        "import matplotlib.pyplot as plt   \n",
        "import pandas as pd  \n",
        "user150 = pd.read_csv(\"top150SplitCompanies.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-76cb759b8d87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0muser150\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"top150SplitCompanies.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'top150SplitCompanies.csv' does not exist: b'top150SplitCompanies.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wLNsoskdwAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################### Decision Tree Regression ###################################\n",
        "# Split training, test set\n",
        "\n",
        "def tree_regression(user):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    topUser = user150[user150['userId'] == user]\n",
        "    X = np.array(topUser.drop(columns = ['rating','userId','vote_average','vote_count']))\n",
        "    y = np.array(topUser['rating'])\n",
        "    y[y < 4] = 0\n",
        "    y[y >= 4] = 1\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=223,\n",
        "                                                    shuffle=True,\n",
        "                                                    stratify = y)\n",
        "\n",
        "    dt_regression = DecisionTreeRegressor(random_state = 123,criterion='mse',max_depth=3) \n",
        "    dt_regression.fit(X_train, y_train)\n",
        "    y_pred = np.array(dt_regression.predict(X_test))\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_pred[i] < np.mean(y_pred):\n",
        "            y_pred[i] = 0\n",
        "        else:\n",
        "            y_pred[i] = 1\n",
        "\n",
        "    num_correct_predictions = (y_pred== y_test).sum()\n",
        "    accuracy = (num_correct_predictions / y_test.shape[0]) * 100\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16c3qDyad8o9",
        "colab_type": "text"
      },
      "source": [
        "### Calculate average accuracy of all the individual user model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yidb3tOld5qG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "userId_list = user150['userId'].unique()\n",
        "accuracy_list = []\n",
        "for i in userId_list:\n",
        "    accuracy_list.append(tree_regression(user = i))\n",
        "    \n",
        "print(\"In decision tree regression model, the average accuracy for all users is \", round(np.mean(accuracy_list),2),\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KBn-ZxseGhQ",
        "colab_type": "text"
      },
      "source": [
        "### Plot Decision Tree regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdJwBxnpeD5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.externals.six import StringIO  \n",
        "from IPython.display import Image  \n",
        "import pydotplus\n",
        "\n",
        "feature_cols = topUser.drop(columns = ['rating','userId','vote_average','vote_count']).columns\n",
        "clf = DecisionTreeRegressor(random_state = 123,criterion='mse',max_depth=3) \n",
        "clf.fit(X_train,y_train)\n",
        "        \n",
        "dot_data = StringIO()\n",
        "export_graphviz(clf, out_file=dot_data,  \n",
        "                filled=True, rounded=True,\n",
        "                special_characters=True,\n",
        "                feature_names = feature_cols)\n",
        "pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "graph.write_png('tree_regression.png')\n",
        "Image(graph.create_png())\n",
        "## value close to 1 means recommend (likely to have a rating >= 4); \n",
        "## value close to 0 means don't recommend (likely to have a rating <4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENzDPcFBegE2",
        "colab_type": "text"
      },
      "source": [
        "# Final Model - KNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1aTDO7BeiwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#all_data = all_data.loc[all_data['userId'] == userid]\n",
        "all_data['class'] = np.where(all_data['rating']< 4, 0, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WTBDplRep-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=223,\n",
        "                                                    shuffle=True,\n",
        "                                                    stratify = y)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN7-ieLvesYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = all_data['class']\n",
        "drop_columns = ['class', 'userId']\n",
        "X = all_data.drop(labels= drop_columns, axis=1)\n",
        "\n",
        "X_train = X_train.loc[:, 'budget':'Documentary']\n",
        "X_train = scale(X_train)\n",
        "X_test = X_test.loc[:, 'budget':'Documentary']\n",
        "X_test = scale(X_test)\n",
        "\n",
        "grid_params_knn = {\n",
        "        'n_neighbors':[*range(2,15,1)],\n",
        "        'weights':['uniform']\n",
        "    }\n",
        "\n",
        "gs_knn = GridSearchCV(\n",
        "         KNeighborsClassifier(),\n",
        "         grid_params_knn,\n",
        "         verbose = 1,\n",
        "         cv = 10,\n",
        "         refit=True,\n",
        "         scoring='accuracy',\n",
        "         n_jobs = -1)\n",
        "\n",
        "gs_knn_results = gs_knn.fit(X_train, y_train)\n",
        "knn_best_score = gs_knn.best_estimator_.score(X_test, y_test)*100\n",
        "#y_knn_predictions = gs_knn.best_estimator_.predict(X_test)\n",
        "print(knn_best_score, '%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34NiLn3qeuY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The best hyperparameter for KNN Model:',gs_knn.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
